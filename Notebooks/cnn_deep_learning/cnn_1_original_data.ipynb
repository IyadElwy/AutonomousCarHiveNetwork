{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2kBbuagGMT6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Drive"
      ],
      "metadata": {
        "id": "yOIHLasFm8wS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUQ43bZIG10T",
        "outputId": "f5f4f435-5969-4e6a-aa8c-57f803dd9364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constant Variables"
      ],
      "metadata": {
        "id": "AhCQaikw2kSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = '/content/gdrive/MyDrive/Projects/Hive Network for autonomous car/src/original_data/dataset_l_r.csv'\n",
        "img_root = '/content/gdrive/MyDrive/Projects/Hive Network for autonomous car/src/original_data'\n",
        "model_path = '/content/gdrive/MyDrive/Projects/Hive Network for autonomous car/src/Models/Model_1'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "UGZ4iGAx2mNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "_tlXHil72e-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "\n",
        "def sharpen_image(image):\n",
        "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
        "    sharpened = cv2.filter2D(image, -1, kernel)\n",
        "    return sharpened\n",
        "\n",
        "\n",
        "def cut_lane_region(image):\n",
        "    return image[150:, :, :]\n",
        "\n",
        "\n",
        "def convert_to_numpy_array(image):\n",
        "  return np.array(image)\n",
        "\n",
        "\n",
        "def convert_to_pil_image(image):\n",
        "  return Image.fromarray(image)\n"
      ],
      "metadata": {
        "id": "d2XyK9S65_ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset\n",
        "class LaneDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.lane_frame = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lane_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.lane_frame['file'][idx]\n",
        "        img_path =  f'{img_root}/{img_path}'\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.lane_frame.iloc[idx, 0]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Define the transforms operation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Lambda(convert_to_numpy_array),\n",
        "    transforms.Lambda(cut_lane_region),\n",
        "    transforms.Lambda(sharpen_image),\n",
        "    transforms.Lambda(convert_to_pil_image),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "# Define the dataset and dataloader\n",
        "lane_dataset = LaneDataset(csv_file=csv_file, transform=transform)\n",
        "batch_size = 32\n",
        "lane_dataloader = DataLoader(lane_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "VZ4uAic-2ect"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and training"
      ],
      "metadata": {
        "id": "NMN5u-Gqm-kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(256 * 7 * 7, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(128, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_j1Q-eUmGUZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "W0Hpc-1W2_3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the training loop\n",
        "def train(model, dataloader, criterion, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'epoch: {epoch}')\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, data in enumerate(dataloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_acc = 100 * correct / total\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "# train(model, lane_dataloader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "fdNzLAz_HQMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "7ddoT9yUOrj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(model_path)"
      ],
      "metadata": {
        "id": "M7_hd1rB9Qp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_file)"
      ],
      "metadata": {
        "id": "hf80Y6XOWmze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sorted(df[df['file'].str.startswith('t7/')]['file'].map(lambda x: int(x.split('/')[1].split('.jpg')[0])).to_list())"
      ],
      "metadata": {
        "id": "N99ti8k_Y5hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "file_path = 't7/64.jpg'\n",
        "\n",
        "image = Image.open(f'{img_root}/{file_path}').convert('RGB')\n",
        "\n",
        "image_tr = transform(image)\n",
        "print(model(image_tr.unsqueeze(dim=0).to(device)).argmax(dim=1).cpu().numpy())\n",
        "print(df[df['file'] == file_path])\n",
        "\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "ccjpgg_ERE1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validating"
      ],
      "metadata": {
        "id": "aeqYB1Qm3V8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, r2_score\n",
        "\n",
        "ground_truth = pd.read_csv(csv_file)\n"
      ],
      "metadata": {
        "id": "8Cky8vVSb5vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "l = []\n",
        "\n",
        "for img in ground_truth['file']:\n",
        "  image = Image.open(f'{img_root}/{img}').convert('RGB')\n",
        "  image_tr = transform(image)\n",
        "  l.append((f'{img}', model(image_tr.unsqueeze(dim=0).to(device)).argmax(dim=1).cpu().numpy()[0]))"
      ],
      "metadata": {
        "id": "s_hmmdjwhpZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = pd.DataFrame(l, columns=['file', 'r'])"
      ],
      "metadata": {
        "id": "DmjYRt6uipsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {accuracy_score(ground_truth['r'], pred['r'])}\")\n",
        "print(f\"Precision: {precision_score(ground_truth['r'], pred['r'])}\")\n",
        "print(f\"Recall: {recall_score(ground_truth['r'], pred['r'])}\")\n",
        "print(f\"R2: {r2_score(ground_truth['r'], pred['r'])}\")"
      ],
      "metadata": {
        "id": "aUOsgtjwlQ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "\n",
        "cf_matrix = confusion_matrix(ground_truth['r'], pred['r'])\n",
        "\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "          zip(group_names,group_counts,group_percentages)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')"
      ],
      "metadata": {
        "id": "0xXQoJD2lkZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Model"
      ],
      "metadata": {
        "id": "lWrGWNjnm5lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model, model_path)\n",
        "# model = torch.load(model_path)\n"
      ],
      "metadata": {
        "id": "zqInw8hymYt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10UdqMvindmw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}